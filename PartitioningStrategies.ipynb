{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Strategies:  RDDs and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Taken from https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "Partitioner: None\n",
      "Partitions structure: [[0], [1], [2], [3, 4], [5], [6], [7], [8, 9]]\n"
     ]
    }
   ],
   "source": [
    "nums = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "rdd = sc.parallelize(nums)\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '57143'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1575928739377'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '10.20.0.195'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism: 8\n",
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums, 2)\n",
    "    \n",
    "print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 15\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [0], [1], [], [2], [3], [], [4], [5], [], [6], [7], [], [8], [9]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums, 15)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because partitionBy() requires data to be in key/value format we will need to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x11e848ef0>\n",
      "Partitions structure: [[(0, 0), (5, 5)], [(1, 1), (6, 6)], [(2, 2), (7, 7)], [(3, 3), (8, 8)], [(4, 4), (9, 9)]]\n",
      "partition: 1 [(0, 0), (5, 5)]\n",
      "partition: 2 [(1, 1), (6, 6)]\n",
      "partition: 3 [(2, 2), (7, 7)]\n",
      "partition: 4 [(3, 3), (8, 8)]\n",
      "partition: 5 [(4, 4), (9, 9)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(5) \\\n",
    "        .persist()\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "j=0\n",
    "for i in rdd.glom().collect():\n",
    "    j+=1\n",
    "    print(\"partition: \" + str(j) + \" \"+ str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now the elements are distributed differently. A few interesting things happened:\n",
    "- `parallelize(nums)` - we are transforming Python array into RDD with no partitioning scheme,\n",
    "- `map(lambda el: (el, el))` - transforming data into the form of a tuple,\n",
    "- `partitionBy(2)` - splitting data into 2 chunks using default *hash partitioner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit assignment of partition locations makes the hashing strategy more apparent.  The use of the `%` function assigns it to the correct partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: [0]: 0 % 2 = partition 0\n",
      "Element: [1]: 1 % 2 = partition 1\n",
      "Element: [2]: 2 % 2 = partition 0\n",
      "Element: [3]: 3 % 2 = partition 1\n",
      "Element: [4]: 4 % 2 = partition 0\n",
      "Element: [5]: 5 % 2 = partition 1\n",
      "Element: [6]: 6 % 2 = partition 0\n",
      "Element: [7]: 7 % 2 = partition 1\n",
      "Element: [8]: 8 % 2 = partition 0\n",
      "Element: [9]: 9 % 2 = partition 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.rdd import portable_hash\n",
    "num_partitions = 2\n",
    "for el in nums:\n",
    "    print(\"Element: [{}]: {} % {} = partition {}\".format(\n",
    "        el, portable_hash(el), num_partitions, portable_hash(el) % num_partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4596069200710135518"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that further analysis will be performed analyzing many similar records within the same country. To optimize network traffic it seems to be a good idea to put records from one country in one node.\n",
    "To meet this requirement, we will need a custom partitioner:\n",
    "Custom partitioner — function returning an integer for given object (tuple key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Dummy implementation assuring that data for each country is in one partition\n",
    "def country_partitioner(country):\n",
    "    return hash(country)\n",
    "    #return portable_hash(country)\n",
    "    \n",
    "\n",
    "# Validate results\n",
    "num_partitions = 5\n",
    "print(country_partitioner(\"Poland\") % num_partitions)\n",
    "print(country_partitioner(\"Germany\") % num_partitions)\n",
    "print(country_partitioner(\"United Kingdom\") % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x11ebadd30>\n",
      "Partitions structure: [[('Poland', {'country': 'Poland', 'amount': 51, 'name': 'Marek'}), ('Germany', {'country': 'Germany', 'amount': 200, 'name': 'Johannes'}), ('Poland', {'country': 'Poland', 'amount': 75, 'name': 'Paul'})], [('United Kingdom', {'country': 'United Kingdom', 'amount': 100, 'name': 'Bob'}), ('United Kingdom', {'country': 'United Kingdom', 'amount': 15, 'name': 'James'})], [], [], []]\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "partition: 1\n",
      "[('Poland', {'country': 'Poland', 'amount': 51, 'name': 'Marek'}), ('Germany', {'country': 'Germany', 'amount': 200, 'name': 'Johannes'}), ('Poland', {'country': 'Poland', 'amount': 75, 'name': 'Paul'})]\n",
      "\n",
      "partition: 2\n",
      "[('United Kingdom', {'country': 'United Kingdom', 'amount': 100, 'name': 'Bob'}), ('United Kingdom', {'country': 'United Kingdom', 'amount': 15, 'name': 'James'})]\n",
      "\n",
      "partition: 3\n",
      "[]\n",
      "\n",
      "partition: 4\n",
      "[]\n",
      "\n",
      "partition: 5\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(5, country_partitioner)\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "print(\"\\n--\\n\")\n",
    "j=0\n",
    "for i in rdd.glom().collect():\n",
    "    j+=1\n",
    "    print(\"\\npartition: \" + str(j) + \"\\n\"+ str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the partitioning scheme, we can now carry out calculations:\n",
    "(We could have also used `forEachPartition`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_sales(iterator):\n",
    "    yield sum(transaction[1]['amount'] for transaction in iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions structure: [[('Germany', {'country': 'Germany', 'amount': 200, 'name': 'Johannes'})], [], [('United Kingdom', {'country': 'United Kingdom', 'amount': 100, 'name': 'Bob'}), ('United Kingdom', {'country': 'United Kingdom', 'amount': 15, 'name': 'James'}), ('Poland', {'country': 'Poland', 'amount': 51, 'name': 'Marek'}), ('Poland', {'country': 'Poland', 'amount': 75, 'name': 'Paul'})]]\n",
      "Total sales for each partition: [200, 0, 241]\n"
     ]
    }
   ],
   "source": [
    "by_country = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(3, country_partitioner)\n",
    "    \n",
    "print(\"Partitions structure: {}\".format(by_country.glom().collect()))\n",
    "\n",
    "# Sum sales in each partition\n",
    "sum_amounts = by_country \\\n",
    "    .mapPartitions(sum_sales) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Total sales for each partition: {}\".format(sum_amounts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [Row(amount=100, country='United Kingdom', name='Bob')], [], [Row(amount=15, country='United Kingdom', name='James')], [Row(amount=51, country='Poland', name='Marek')], [], [Row(amount=200, country='Germany', name='Johannes')], [Row(amount=75, country='Poland', name='Paul')]]\n",
      "partition: 1\n",
      "[]\n",
      "partition: 2\n",
      "[Row(amount=100, country='United Kingdom', name='Bob')]\n",
      "partition: 3\n",
      "[]\n",
      "partition: 4\n",
      "[Row(amount=15, country='United Kingdom', name='James')]\n",
      "partition: 5\n",
      "[Row(amount=51, country='Poland', name='Marek')]\n",
      "partition: 6\n",
      "[]\n",
      "partition: 7\n",
      "[Row(amount=200, country='Germany', name='Johannes')]\n",
      "partition: 8\n",
      "[Row(amount=75, country='Poland', name='Paul')]\n",
      "\n",
      "After 'repartition()'\n",
      "Number of partitions: 50\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(amount=200, country='Germany', name='Johannes')], [], [Row(amount=51, country='Poland', name='Marek'), Row(amount=75, country='Poland', name='Paul')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')], [], [], [], []]\n",
      "+------+--------------+--------+\n",
      "|amount|       country|    name|\n",
      "+------+--------------+--------+\n",
      "|   200|       Germany|Johannes|\n",
      "|    51|        Poland|   Marek|\n",
      "|    75|        Poland|    Paul|\n",
      "|   100|United Kingdom|     Bob|\n",
      "|    15|United Kingdom|   James|\n",
      "+------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "rdd = spark.sparkContext \\\n",
    "        .parallelize(transactions) \\\n",
    "        .map(lambda x: Row(**x))\n",
    "    \n",
    "df = spark.createDataFrame(rdd)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df.rdd.glom().collect()))\n",
    "\n",
    "\n",
    "j=0; \n",
    "for i in df.rdd.glom().collect():\n",
    "    j+=1\n",
    "    print(\"partition: \" + str(j) + \"\\n\"+ str(i))\n",
    "\n",
    "\n",
    "\n",
    "# Repartition by column\n",
    "df2 = df.repartition(50,\"country\")\n",
    "\n",
    "print(\"\\nAfter 'repartition()'\")\n",
    "print(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(df2.rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))\n",
    "\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `coalesce()` and `repartition()`\n",
    "\n",
    "If you are increasing the number of partitions use repartition()(performing full shuffle),\n",
    "if you are decreasing the number of partitions use coalesce() (minimizes shuffles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 8\n",
      "Partitions structure: [[Row(num=0)], [Row(num=1)], [Row(num=2)], [Row(num=3), Row(num=4)], [Row(num=5)], [Row(num=6)], [Row(num=7)], [Row(num=8), Row(num=9)]]\n",
      "Number of partitions: 4\n",
      "Partitions structure: [[Row(num=4), Row(num=9)], [], [], [Row(num=0), Row(num=1), Row(num=2), Row(num=3), Row(num=5), Row(num=6), Row(num=7), Row(num=8)]]\n"
     ]
    }
   ],
   "source": [
    "nums_rdd = spark.sparkContext \\\n",
    "        .parallelize(nums) \\\n",
    "        .map(lambda x: Row(x))\n",
    "    \n",
    "nums_df = spark.createDataFrame(nums_rdd, ['num'])\n",
    "\n",
    "print(\"Number of partitions: {}\".format(nums_df.rdd.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(nums_df.rdd.glom().collect()))\n",
    "\n",
    "nums_df = nums_df.repartition(4)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(nums_df.rdd.getNumPartitions()))\n",
    "print(\"Partitions structure: {}\".format(nums_df.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations that benefit from partitioning\n",
    "All operations performing shuffling data by key will benefit from partitioning. \n",
    "\n",
    "Some examples are \n",
    "- `cogroup()`, \n",
    "- `groupWith()`, \n",
    "- `join()`, \n",
    "- `leftOuterJoin()`, \n",
    "- `rightOuterJoin()`, \n",
    "- `groupByKey()`, \n",
    "- `reduceByKey()`, \n",
    "- `combineByKey()`\n",
    "- `lookup()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning strategy is not always preserved under transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x11c2d4b38>\n",
      "Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]\n",
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[(0, 0), (2, 4), (4, 8), (6, 12), (8, 16)], [(1, 2), (3, 6), (5, 10), (7, 14), (9, 18)]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(2) \\\n",
    "        .persist()\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "# Transform with `map()` \n",
    "rdd2 = rdd.map(lambda el: (el[0], el[0]*2))\n",
    "\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd2.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd2.partitioner))  # We have lost a partitioner\n",
    "print(\"Partitions structure: {}\".format(rdd2.glom().collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, there are some functions provided that guarantee that each tuple’s key remains the same — `mapValues()`, `flatMapValues()` or `filter()` (if the parent has a partitioner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x11bebd710>\n",
      "Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]\n",
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x11bebd710>\n",
      "Partitions structure: [[(0, 0), (2, 4), (4, 8), (6, 12), (8, 16)], [(1, 2), (3, 6), (5, 10), (7, 14), (9, 18)]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(2) \\\n",
    "        .persist()\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "# Use `mapValues()` instead of `map()` \n",
    "rdd2 = rdd.mapValues(lambda x: x * 2)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd2.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd2.partitioner))  # We still got partitioner\n",
    "print(\"Partitions structure: {}\".format(rdd2.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
